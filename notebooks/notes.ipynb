{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "- https://scikit-learn.org/stable/modules/classes.html \n",
    "- https://spark.apache.org/docs/1.2.2/api/python/pyspark.mllib.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "- Regression Methods\n",
    "- Bias-Variance Trade-Off\n",
    "- Validation and Cross-Validation\n",
    "- Hyperparameter Tuning\n",
    "- Generalized Linear Methods\n",
    "- Tree-Based Methods\n",
    "- Other Classifiers\n",
    "- K-Nearest Neighbors (KNN)\n",
    "- Support Vector Machines (SVM)\n",
    "- Naive Bayes\n",
    "- Linear/Quadrant Discriminant Analysis (LDA/QDA)\n",
    "- Assessing Performance\n",
    "- Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "We can use grid search or randomized search to search for the optimal hyperparameter combination. It is more preferable to use randomized search when the hyperparameter space is large. Instead of trying out all possible combinations, it evaluates a given number of random combinations by selecting a random hyperparameter value at every iteration. \n",
    "\n",
    "## Validation Set + Cross-Validation\n",
    "\n",
    "After we have divided our dataset into a training and test set, we have to think split our training set into 2 more parts, one for training and one for validation. We need a validation set because it is the closest estimate of the test error without having to touch the test set. \n",
    "\n",
    "![alt text](https://miro.medium.com/max/1400/1*AAwIlHM8TpAVe4l2FihNUQ.png)\n",
    "\n",
    "Cross validation works as follows, \n",
    "\n",
    "1. Split the training set into $k$ chunks of data. \n",
    "2. For each model with set of hyperparameters and complexity, \n",
    "    - Train the model on $k - 1$ chunks and leave the $i$th chunk for validation\n",
    "    - Calculate the error of the model on the validation set\n",
    "3. Average the validation set error over all the $k$ chunks\n",
    "\n",
    "In the case where we are tuning $m$ hyperparameters, and using an n-fold cross validation, we are doing m*n training rounds. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Motivation\n",
    "- It is easy to implement, interpret and efficient to train\n",
    "- It makes no asumptions about the distributions of classes in the feature space\n",
    "- It can easily extend to a multi-class application\n",
    "- It quantifies the class assignments with probability estimates\n",
    "- It is less inclined to overfitting, but can happen in high dimensional dataset\n",
    "\n",
    "### Disadvantages\n",
    "- If the number of observations is less than the number of features, there can be overfitting\n",
    "- It assumes that there is only a linear relationship between the explanatory and response variables. This means data that fit better with non-linear boundaries won't be well-predicted with logistic regression. \n",
    "- It also assumes that explanatory variables are linearly related to the logit. \n",
    "- It requires no multicollinearity between independent variables\n",
    "\n",
    "### Summary\n",
    "- Logistic regression models the probability that an observation belongs to a particular category. \n",
    "- **How?** Like linear regression, it computes a weighted sum of the input features and outputs the logistic of it, instead of a continuous value. \n",
    "    - It does so by inserting the weighted sum into a logistic function. \n",
    "    - **Why a logistic/sigmoid function?** If we simply use the weighted sum to assign a probability on an observation, it wouldn't make sense as **probabilities have to fall between [0, 1]**. A logistic function outputs values between [0, 1].  \n",
    "- **How does logistic regression calcuate the weights/coefficients of the regression equation?** \n",
    "    - Finds the set of coefficients/weights such that the probability of the observation belonging to a particular category corresponds as closely to its actual category by maximizing the likelihood or minimizing the cross-entropy loss function. Gradient descent is employed to find the global minimum of that function, and the result is the set of weights/coefficients that best models the probability.\n",
    "\n",
    "### Assumptions\n",
    "- Response variable is binary\n",
    "- Observations are independent, meaning we can maximize the likelihood to find the best fitting set of weights (Optimizer)\n",
    "- There shouldn't be any multicollinearity among variables because when two or more explanatory variables are highly correlated to each other, they don't provide unique or independent information in the regression model\n",
    "- There shouldn't be any extreme outliers \n",
    "- There is a linear relationship between the explanatory variables and the logit\n",
    "\n",
    "### Hyperparameters\n",
    "- When used with L1 or L2 regularization, logistic regression inherits hyperparameter tuning from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVMs)\n",
    "\n",
    "### Motivations\n",
    "- SVMs are effective in high dimensional data\n",
    "- It is still effective even when number of dimensions is greater than the number of samples\n",
    "- It is also memory efficient because SVMs only use a subset of training observations in the decision functions (support vectors)\n",
    "- Versatile: different kernel functions can be used for the decision function to create a linear or non-linear decision boundary\n",
    "\n",
    "### Disadvantages\n",
    "- If number of features is much greater than number of observations, we need to avoid overfitting by choosing the optimal kernel function and regularization term\n",
    "- SVMs don't directly quantify the probability of class assignments\n",
    "\n",
    "### Summary\n",
    "\n",
    "### What is a hyperplane?\n",
    "\n",
    "In a p-dimensional space, a hyperplane is **p-1 dimensional flat affine subspace**. For instance, in a 2 dimensional space, the hyperplane is a line (1 dimensional flat affine subspace). \n",
    "\n",
    "A hyperplane is characterized by the following equation \n",
    "\n",
    "$$\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p = 0$$\n",
    "\n",
    "where $\\beta = (\\beta_0, \\beta_1, \\ldots, \\beta_p)$ are coefficients/weights and $X = (X_1, X_2, ..., X_p)$ is a point in the space. We can also see that this is the inner product of $\\beta$ and $X$/weighted sum of the features constrained to being equal to 0. \n",
    "\n",
    "Now, if the inner product is $> 0$, this indicates that $X$ lies on one side of the hyperplane and vice versa. We can think of the hyperplane as dividing the p-dimensional space into 2 halves. \n",
    "\n",
    "With this property, we can define a **separating hyperplane** to classify points based on which side of the hyperplane it is located, which is characterized by the sign of the inner product result. \n",
    "\n",
    "If the target value of an observation is 1, then the inner product is $> 0$ and if the target value is -1, the inner product is $< 0$. This gives way to the property\n",
    "\n",
    "$$y_i(\\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\ldots + \\beta_pX_{ip}) > 0$$\n",
    "\n",
    "We can also utilize the magnitude of inner productâ€“a value far from 0 means it lies far from the hyperplane and we can be confident about our class prediction, and a value closer to 0 means we are more uncertain about our prediction. \n",
    "\n",
    "### Maximal Margin Classifier\n",
    "\n",
    "Data can be separated by an infinite number of hyperplanes because they can be shifted or rotated without changing anything. \n",
    "\n",
    "To choose the hyperplane to separate the data most accurately, we can use the maximal margin hyperplane. The margin is the smallest perpendicular distance between the observations and hyperplane. \n",
    "\n",
    "With that, the maximal margin hyperplane is the separating hyperplane where the margin is the largest/has the farthest minimum distance to the observations. In an easier sense, we can think of the maximal margin hyperplane to be the midline on the widest \"slab\" that we can insert between 2 classes on the data. We can then use this separating hyperplane as our maximal margin classifier.  \n",
    "\n",
    "#### How can we construct the maximal margin classifier?\n",
    "\n",
    "For a set of n observations $x_1, \\ldots, x_n \\in \\R^p$ and is associated with class labels $y_1, \\ldots, y_n \\in {-1, 1}$, the solution to the optimization problem for the maximal margin is \n",
    "\n",
    "$$\n",
    "\\text{maximize}_{\\beta_0, \\ldots, \\beta_p, M} M \\\\\n",
    "\\text{subject to} \\sum_{j = 1}^{p}\\beta_{j}^2 = 1 \\\\\n",
    "y_i(\\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\ldots + \\beta_pX_{ip}) \\geq M \\forall i = 1, \\ldots, n \n",
    "$$\n",
    "\n",
    "Here, $M$ is the margin. Hence, the first equation states that we want to maximize the margin by optimizing on the $\\beta$\n",
    "\n",
    "The last constraint guarantees that each observation will be on the correct side of the hyperplane, as in the inner product value that's at least greater than the margin can either be on the margin or hyperplane of the correct side. This is from the property of the separating hyperplane where $y_i(\\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\ldots + \\beta_pX_{ip}) > 0$ where we were guaranteeing that the observation would simply be on the right side of the hyperplane. By making it $\\geq M$, we are making it stricter and saying the observation will also be on the right side of the margin. \n",
    "\n",
    "The second constraint allows the inner product in the last constraint to be the perpendicular distance between each observation and separating hyperplane. \n",
    "\n",
    "Together, the solution allows us to guarantee that each observation will be on the correct side of the hyerplane and at least a distance M from the hyerplane. \n",
    "\n",
    "### Support Vector Classifiers / Soft Margin Classifier\n",
    "\n",
    "There are many cases where a separable hyperplane won't exist, and even if there was a classifier that perfectly classifies all of the observations can lead to an overfitting classifier, becoming extremely sensitive to a change in the data. \n",
    "\n",
    "The support vector classifier is the generalization of the maximal margin classifier where we use a hyperplane that *almost* separates the classes, using a **soft margin**. This soft margin allows some observations to be on the incorrect side of the margin or hyperplane. \n",
    "\n",
    "#### How can we construct the support vector classifier / soft margin classifier?\n",
    "\n",
    "$$\n",
    "\\text{maximize}_{\\beta_0, \\ldots, \\beta_p, \\epsilon_1, \\ldots, \\epsilon_n, M} M \\\\\n",
    "\\text{subject to} \\sum_{j = 1}^{p}\\beta_{j}^2 = 1 \\\\\n",
    "y_i(\\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\ldots + \\beta_pX_{ip}) \\geq M(1 - \\epsilon_i) \\forall i = 1, \\ldots, n \\\\\n",
    "\\epsilon_i \\geq 0, \\sum_{i = 1}^{n}\\epsilon_i \\leq C\n",
    "$$\n",
    "\n",
    "$\\epsilon_1, \\ldots, \\epsilon_n$ are slack variables that allow observations to be on the wrong side of the margin or hyperplane as it describes where each observation is located relative to the hyperplane and margin. \n",
    "\n",
    "If $\\epsilon_i = 0$, the observation is on the correct side of the margin, if $\\epsilon_i > 0$, the observation is on the wrong side of the margin and if $\\epsilon_i > 1$, the observation is on the wrong side of the hyperplane. \n",
    "\n",
    "$C$ is a tuning parameter, which bounds the sum of the slack variables. It determines the number and severity of the violations to the margin (and hyperplane) that we tolerate.\n",
    "\n",
    "For $C = 0$, $\\epsilon_1 = \\ldots = \\epsilon_n = 0$, which is the maximal margin hyperplane classifier. For $C > 0$, no more than $C$ observations can be on the wrong side of the hyperplane. As C increases we become more tolerant of violations to the margin and the margin will widen and vice versa. \n",
    "\n",
    "In the solution of the optimization problem above, actually only observations that lie on the marin or that violate the margin will affect the hyperplane and the classifier. These observations are called **support vectors**. \n",
    "\n",
    "Because the support vector classifier only depends on a potentially small subset of observations, it implies that the classifier is robust to observations that are far away from the hyperplane (outliers). \n",
    "\n",
    "#### C + Bias-Variance Trade-off\n",
    "Like many other tuning parameters, $C$ controls the bias-variance trade-off. When $C$ is small, we get narrow margins where not many observations violate them, but we get a classifier that is highly fit to the data and can lead to a classifier that has low bias and high variance and vice versa. This means we want to choose a $C$ that doesn't lead our model to overfit or underfit. \n",
    "\n",
    "### Support Vector Machine (SVM)\n",
    "\n",
    "The support vector classifier and maximal margin classifier are linear clasifiers that product linear decision boundaries. The support vector machine is a more generalized version of these 2 classifiers that supports non-linear decision boundaries. \n",
    "\n",
    "For data that work better with non-linear boundaries, we can enlarge the feature space using functions of predictors with **kernels**. \n",
    "\n",
    "#### Linear SVC\n",
    "\n",
    "The solution to the support vector classifier is just the inner products of the observations, not simply just the observations. The linear support vector classifier is \n",
    "\n",
    "$$\n",
    "f(x) = \\beta_0 + \\sum_{i = 1}^{n}\\alpha_i\\langle x, x_i \\rangle\n",
    "$$\n",
    "\n",
    "We can think of the inner product between $x$ (test observation) and $x_i$ (training observation) as the similarity between each other. To estimate $\\alpha_i$, all we need is just the inner products of the observations. \n",
    "\n",
    "$\\alpha_i$ is actually non-zero only for support vectors in the solution. This makes sense because only support vectors affect the classifier. Thus, it is only necessary we know the inner product/similarity between $x$ and the support vectors to know which side it should be on/class assignment. \n",
    "\n",
    "Take S as the collection of indices of the support vectors, we can rewrite the equation as \n",
    "\n",
    "$$\n",
    "f(x) = \\beta_0 + \\sum_{i \\in S}\\alpha_i\\langle x, x_i \\rangle\n",
    "$$\n",
    "\n",
    "Overall, the inner products of the observations represent the linear support vector classifier and computing the coefficients $\\alpha_i$ or $\\beta_i$.\n",
    "\n",
    "#### Non-Linear SVC/SVM\n",
    "\n",
    "Now instead, of the inner product representing which side the observation should be on, we can use the kernel which is function that quantifies the similarity of 2 observations. The non-linear support vector classifier/support vector machine is characterized as\n",
    "\n",
    "$$\n",
    "f(x) = \\beta_0 + \\sum_{i \\in S}\\alpha_iK(x, x_i)\n",
    "$$\n",
    "\n",
    "If K is simply the inner product/Pearson standard correlation (linear kernel), we get the support vector classifier. If we expand from that and use a polynomial kernel, we get more flexible decision boundaries. \n",
    "\n",
    "Rather than simply enlarging the feature space using functions of original features, we don't need to work with a lot of dimensions and only need to compute the kernel values for all $n \\choose 2$ distinct pairs of observations. \n",
    "\n",
    "### Relationship to Logistic Regression\n",
    "\n",
    "### Practical Procedure\n",
    "\n",
    "#### Data Preprocessing\n",
    "\n",
    "- SVM requires that each observation is represented by a vector of real numbers. For categorical features, we need to encode them as one-hot numbers or integers to convert them to real numbers\n",
    "\n",
    "- Scaling before SVM is very important as to avoid attributes in greater numeric ranges dominating those in smaller numeric ranges. It is also to avoid numerical difficulties during calculations. Since kernel values depend on inner product of feature vectors, large attribute values or the fact that they're even differing ranges can cause problems. \n",
    "\n",
    "#### Model Selection\n",
    "\n",
    "RBF kernel is a good first choice as it nonlinearly maps samples into higher dimensional spaces to support relationships between features that are nonlinear. However, we may want to avoid it when we're dealing with high dimensional data. \n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Depending on the kernel we use, there can be different hyperparameters. However, the main hyperparameter that we have to tune is $C$, the maximum number of observations that are allowed to violate the margin/tolerance for violation of the margin. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "\n",
    "### Motivations\n",
    "- Can make highly accurate predictions\n",
    "- Intuitive to understand\n",
    "- Can learn non-linear decision boundaries for classification\n",
    "- No training step required, all the work happens during prediction of class labels\n",
    "- Simple hyperparameter tuning. Only need to tune K (number of nearest neighbors to account for). \n",
    "- KNN is a non-parametric algorithm. This means it is a good algorithm to use when we have little to no information on the data distribution. \n",
    "\n",
    "### Disadvantages \n",
    "- Has high complexity for large datasets and high-dimensional data\n",
    "- It assumes equal importance across all features. This also means that KNN is sensitive if the features have different ranges and that we should avoid unrelated features. \n",
    "- It is sensitive to outliers. Because class labels are assigned on the basis of the distance between the main data point and its K neighbors, having outliers can significantly affect the class assignments and cause the result to be more inaccurate. Outliers also affect KNN when the dataset has many dimensions. The average separation tends to be higher for high dimensions, and this impacts the final result. \n",
    "- KNN doesn't perform well on imbalanced datasets. If there is a class majority, the algorithm is bound to give more preference to A. \n",
    "- It can't deal with missing values. \n",
    "- Redundancy affects performance. If 2 or more variables are correlated with each other, the algorithm will place less weights on them without any legitimate reason to actually do that. \n",
    "\n",
    "### Hyperparameters\n",
    "- K: Number of nearest neighbors a data point takes into account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree-based Methods\n",
    "\n",
    "Tree-based methods involve segmenting the predictor space (values of predictor variables) into a number of regions based on a set of splitting rules, and predictions are made for a given observation based on the mean or mode of the response values for observations in the region it belongs to.\n",
    "\n",
    "### Motivations\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "### Decision Trees\n",
    "\n",
    "Decision trees can be used for regression or classification tasks. To build decision trees, \n",
    "1. Divide the predictor space into $J$ distinct and non-overlapping regions $R_1, \\ldots, R_j$ by making splits based on predictor feature values\n",
    "2. For every observation that falls into a region $R_j$, we make the same prediction based on the mean of response values of observations (regression)/most commonly occuring class in that region (classification).\n",
    "\n",
    "#### Regression Trees\n",
    "\n",
    "To build a regression tree, we optimize on minimizing the RSS (residual sum of squares) for each split/region. The RSS is defined as $\\sum_{i \\in R_j}(y_i - \\hat{y}_{R_j})^2$ and can be interpreted as **the deviation from the predicted reponse to the actual response value a region/split**. This optimization is a top-down, greedy approach because we begin spltting from the top of the tree and at each step of building the tree, we try to make the split that results in the lowest RSS. We don't care about making a split that will give a better tree in the future. We consider all predictors and all possible split points for each of the predictors at each step and choose the pair that results in the lowest RSS. For a single split, we can think of it as getting 2 regions and we will have to use the sum of the RSS from the 2 regions as our RSS to optimize on. \n",
    "\n",
    "$$R_1(j, s) = \\{X | X_j < s\\} \\text{ and } R_2(j, s) = \\{X | X_j \\geq s\\} \\\\ \\text{minimize} \\sum_{i: x_i \\in R_1(j, s)}(y_i - \\hat{y}_{R_1})^2 + \\sum_{i: x_i \\in R_2(j, s)}(y_i - \\hat{y}_{R_2})^2 $$ \n",
    "\n",
    "The next split will be on one of the previous splits instead of the entire predictor space. Again, the choice will be decided by optimizing on RSS. This splitting is called **recursive binary splitting**.\n",
    "\n",
    "#### Classification Trees\n",
    "\n",
    "To build a classification tree, we don't optimize on RSS as it doesn't work with categorical variables. Instead, we can optimize on the classification error rate, Gini index or entropy. \n",
    "\n",
    "Since we plan to assign an observation the class that appears the most in that region, the classification error rate is a natural choice as it is the fraction of observations in that region that don't belong to the most common class. This way, we try to get a region that has observations that are most similar with each other. \n",
    "\n",
    "However, classification error rate can be a naive metric and not sensitive enough. We can use the Gini index which is a measure of total variance across observations of the K classes. $\\hat{p}_{mk}$ here is the proportion of observations that are in the kth class of the mth region. Since our goal is to make a split where it results in a region where observations are dominated by a class assignment, the Gini index measures node/split/region purity. \n",
    "\n",
    "If all of $\\hat{p}_{mk}$ are close to 0 or 1, and ideally one $\\hat{p}_{mk}$ is close to 1 and others close to 0 can give the Gini index a small value. A small value indicates the high node/split purity. \n",
    "\n",
    "$$G = \\sum_{k = 1}^{K}\\hat{p}_{mk}(1 - \\hat{p}_{mk})$$\n",
    "\n",
    "Entropy is an alternative to the Gini index and like it, entropy will take on a small value if the mth node/split/region is pure. \n",
    "\n",
    "#### Which quality metric to choose?\n",
    "\n",
    "#### Tree Pruning\n",
    "\n",
    "### Ensemble Methods \n",
    "\n",
    "An ensemble method is an approach that combines many simple \"building block\" models that lead to mediocre predictions, **weak learners**, to product a single powerful model. \n",
    "\n",
    "#### Bagging / Bootstrap aggregation\n",
    "\n",
    "Decision trees suffer from high variance, meaning it can give vastly different predictions to different data. Also recall, a high variance model means the model is overfitting to the data and can perform very badly on unknown data. \n",
    "\n",
    "An approach with low variance can yield similar results if applied repeatedly to distinct datasets. By taking advantage of this and bootstrapping, we can reduce the variance of a model and improve its performance. \n",
    "\n",
    "Recall that for a set of n independent observations each with a variance $\\sigma^2$, the variance of the mean of observations is $\\sigma^2/n$. It is apparent that**averaging over a set of observations reduces variance**. \n",
    "\n",
    "From here, it is natural to collect many datasets, build a model for each of the bootstrapped datasets and then average the predictions from those models. For classification more specifically, we can simply take the most commonly occurring class among the trees' predictionsâ€“the **majority vote**.\n",
    "\n",
    "To get distinct datasets, we can use bootstrapping to take repeated (with replacement) samples from a single dataset. The trees applied on these bootstraped datasets are grown deep and not pruned. Overall, we can see that even though each individual tree has high variance, once we average over these trees, we can reduce the variance. \n",
    "\n",
    "This process happens for each observation in the original dataset to assign a class assignment/give a predicted value. \n",
    "\n",
    "##### Out-of-Bag (OOB) Error Estimation \n",
    "\n",
    "For a bagged model, we don't need to use cross-validatioin or validation set to validate our model.\n",
    "\n",
    "On average, each tree in a bagged model uses around 2/3 of observations in the original dataset. The remaining 1/3 not used to train a tree are Out-of-Bag (OOB) observations. \n",
    "\n",
    "For an observation, we can predict the response/class of it using trees where the observation is OOB/not in the bootstrapped dataset. This will yield 1/3 of the number of trees we use to train a tree. Like training a tree, we can average the predicted responses or take a majority vote. This can be repeated for all observations in the original observations, and we can take the overall OOB MSE (regression) or OOB classification error as a performance metric to estimate the test error. \n",
    "\n",
    "This works because we're only using trees that were not fit using the observation of interest. OOB is useful when we are dealing with large datasets and cross-validation can be expensive. \n",
    "\n",
    "#### Random Forest\n",
    "\n",
    "Random forest is an upgraded version of the bagging ensemble method whereby it decorrelates the bagged trees.\n",
    "\n",
    "Unlike bagging, each split considered in a decision tree uses a random sample of $m$ predictors as split candidates from the full set of $p$ predictors. A fresh sample of $m$ predictors is considered at each split, and typically $m$ is approximately $\\sqrt{p}$. This means that each split can't even consider a majority of the predictors. \n",
    "\n",
    "##### How can trees be correlated with each other?\n",
    "\n",
    "Suppose we have a strong predictor along with a few other moderately strong predictors. When we build a model that uses bagged trees, the bagged trees can \"look\" quite similar to each other because they'll most likely use the strong predictor as the top split. The predictions of these bagged trees will be highly correlated and will not lead to as large of a reduction in variance compared to averaging over uncorrelated predictions.\n",
    "\n",
    "##### How does Random Forest overcome the shortcomings of bagging?\n",
    "\n",
    "Random forests forces each split in a bagged tree to consider only a subset of predictors. On average, only $(p - m)/p$ will not even consider the strong predictor and other predictors have a chance. More importantly, this decorrelates the trees and the predictions. The average of the predictions from bagged trees is more reliable.\n",
    "\n",
    "#### Boosting (Additive Modeling)\n",
    "\n",
    "![alt text](https://miro.medium.com/max/1400/1*QJZ6W-Pck_W7RlIDwUIN9Q.jpeg)\n",
    "\n",
    "Unlike bagging, boosting involves the trees being grown sequentially: each tree is using information from previously grown trees. Types of boosting models include AdaBoost, Gradient Boosting, XGBoost. \n",
    "\n",
    "##### AdaBoost\n",
    "\n",
    "For an AdaBoost classifier, the algorithm first trains a base classifier on the dataset and makes a prediction. The algorithm then increases the relative weight of misclassified observations. Then, a second classifier is trained on the modified dataset, now using the updated weights and makes predictions. This process repeats until a cutoff. \n",
    "\n",
    "##### Gradient Boosting\n",
    "\n",
    "In gradient boosting, we fit a tree to the residuals from the current model rather than its outcome as the response. We then add this new decision tree to the fitted function (entire model) to update the residuals of the entire model. By fitting the small trees to the residuals, we can slowly improve the fitted function in areas where it doesn't perform well because we're trying to find a relationship between the predictor variables and the residuals. \n",
    "\n",
    "##### Hyperparameters\n",
    "- Number of trees. Unlike bagging and random forests, boosting can overfit if number of trees is too large. \n",
    "- Shrinkage parameter, a small positive number. This controls the rate the boosting learns. A small shrinkage parameter can require a large number of trees to achieve good performance. \n",
    "- Number of splits in each tree. This controls the complexity of the boosted ensemble. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating classifiers\n",
    "\n",
    "When evaluating classifiers, we can employ validation or cross-validation to validate our model and assess how it performs on a dataset that's closer to future/unknown data. \n",
    "\n",
    "However, another big problem plagues the evaluation of classifiers: Class imbalance. \n",
    "\n",
    "Imagine we made a naive classifier for detecting spam where it always predicts an email is spam and that our dataset is actually 90% of spam emails. The accuracy of the classifier will be 90% because most of the emails are actually spam. This is called a **majority class classifier**, a simple classifier that can have a high accuracy if there is a class imbalance. \n",
    "\n",
    "Hence, accuracy might not be the most robust and reliable metric to evaluate a classifier. That's why we have other ways of evaluating them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix - Precision/Recall\n",
    "\n",
    "![alt text](https://glassboxmedicine.files.wordpress.com/2019/02/confusion-matrix.png?w=816)\n",
    "\n",
    "For a binary classifier, each row represents an **predicted class** and each column represents a **actual class**. A confusion matrix gives information on correctly and incorrectly predicted values with reference to values' actual classes in a summarized form. \n",
    "\n",
    "From a confusion matrix, we can derive **precision**: correctly predicted positives/all predicted positives and **recall/sensitivity/true positive rate (TPR)**: correctly predicted positives/all actual positives. \n",
    "\n",
    "![alt text](https://miro.medium.com/max/1400/1*yH2SM0DIUQlEiveK42NnBg.png)\n",
    "\n",
    "For multiclass classifier, we can obtain the same information as a confusion matrix for a binary classifier, but **for each class**. This means we can find the true/false positives/negatives for each class and we can transform the multiclass problem to a binary problem by thinking in a **One-versus-Rest** way. \n",
    "\n",
    "Focusing on the Apple class, TP = 7, TN = (2+3+2+1) = 8, FP = (8+9) = 17, FN = (1+3) = 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs K-fold cross-validation but will return the predicted made on each test fold\n",
    "y_train_pred = model_selection.cross_val_predict(estimator, X_train, y_train, cv)\n",
    "metrics.confusion_matrix(y_train, y_train_pred)\n",
    "\n",
    "# Calculate precision and recall score\n",
    "metrics.precision_score(y_train, y_train_pred)\n",
    "metrics.recall_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall - F1 Score\n",
    "\n",
    "Sometimes, it can be convenient to combine precision and recall into a single metricâ€“**F1 score**, particularly if we need a simple way to **compare 2 classifiers**. \n",
    "\n",
    "F1 score favors classifiers that have similar precision and recall, so it can be fitting to use it when we want to optimize for both precision and recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision/Recall Trade-off\n",
    "\n",
    "Suppose the classifier is deciding whether a potential client is risky or not based on their income against an income threshold. The precision/recall trade-off can be shown when we shift the decision threshold and the precision and recall changes. \n",
    "\n",
    "To visualize this tradeoff, we can plot precision and recall values against threshold or even plot precision against recall. \n",
    "\n",
    "![alt text](https://miro.medium.com/max/1248/1*TqzfzabXrej1FTdZuNNYIQ.png)\n",
    "\n",
    "### Precision-Recall Curve\n",
    "\n",
    "Precision-Recall curves plot precision against recall where the precision and recall values are taken from decision thresholds from either a decision function or probability estimates of the positive class.\n",
    "\n",
    "The curve allows us to choose a decision threshold that defines the precision/recall tradeoff we make. \n",
    "\n",
    "![alt text](https://machinelearningmastery.com/wp-content/uploads/2020/01/Precision-Recall-Curve-of-a-Logistic-Regression-Model-and-a-No-Skill-Classifier2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives list of precision, recall and threshold values that can be used to plot a precision-recall curve\n",
    "# y_scores can be scores from a decision function or probability estimates of the positive class\n",
    "precisions, recalls, thresholds = metrics.precision_recall_curve(y_train, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "\n",
    "Receiver operating characteristic curves are like precision-recall curves but they plot the true positive rate (**recall**) against the false positive rate, the ratio of negative instances that are incorrectly classified as positive (1 - true negative rate (specificity)). \n",
    "\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/Roc_curve.svg/1920px-Roc_curve.svg.png)\n",
    "\n",
    "One way to compare classifiers is to measure the area under the curve (AUC). A perfect classifier will have an AUC of 1 whereas a random classifier will have a ROC of 0.5. \n",
    "\n",
    "### ROC Curve vs. Precision-Recall (PR) Curve?\n",
    "Rule of thumb, PR curve is prefered when there aren't a lot of positive class values or when we care more about false positives than false negatives. Otherwise, use ROC curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives list of fpr, tpr and threshold values that can be used to plot a ROC curve\n",
    "# y_scores can be scores from a decision function or probability estimates of the positive class\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "### The main idea of gradient descent is to tweak the parameters iteratively to minimize a cost function.\n",
    "\n",
    "1. General Idea\n",
    "2. Batch Gradient Descent\n",
    "3. Stochastic Gradient Descent\n",
    "4. Mini-batch Gradient Descent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('pax_satisfaction')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4c148714c4b36448fc626c7d0cced70895e17adbe163c475e562080f31f1b22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
