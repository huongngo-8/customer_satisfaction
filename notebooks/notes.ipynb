{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "- https://scikit-learn.org/stable/modules/classes.html \n",
    "- https://spark.apache.org/docs/1.2.2/api/python/pyspark.mllib.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. What is regression? \n",
    "2. Assumptions of regression\n",
    "\n",
    "### Simple Linear Regression\n",
    "\n",
    "### Multiple Linear Regression\n",
    "\n",
    "### Generalized Linear Models\n",
    "\n",
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "## Introduction\n",
    "1. What is classification? \n",
    "2. Why not linear regression? \n",
    "\n",
    "## Classifiers \n",
    "\n",
    "3. Logistic Regression\n",
    "4. K-Nearest Neighbors (KNN) \n",
    "5. Support Vector Machines \n",
    "6. Tree Methods\n",
    "7. Naive Bayes (NB)\n",
    "8. Linear/Quadrant Discriminant Analysis (LDA/QDA)\n",
    "9. How multi-class classification can be a binary classification problem\n",
    "\n",
    "## Evaluating classifiers\n",
    "1. Confusion Matrix\n",
    "2. Precision and Recall\n",
    "3. F1-Score\n",
    "4. ROC Curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Motivation\n",
    "- It is easy to implement, interpret and efficient to train\n",
    "- It makes no asumptions about the distributions of classes in the feature space\n",
    "- It can easily extend to a multi-class application\n",
    "- It quantifies the class assignments with probability estimates\n",
    "- It is less inclined to overfitting, but can happen in high dimensional dataset\n",
    "\n",
    "### Disadvantages\n",
    "- If the number of observations is less than the number of features, there can be overfitting\n",
    "- It assumes that there is only a linear relationship between the explanatory and response variables. This means data that fit better with non-linear boundaries won't be well-predicted with logistic regression. \n",
    "- It also assumes that explanatory variables are linearly related to the logit. \n",
    "- It requires no multicollinearity between independent variables\n",
    "\n",
    "### Summary\n",
    "- Logistic regression models the probability that an observation belongs to a particular category. \n",
    "- **How?** Like linear regression, it computes a weighted sum of the input features and outputs the logistic of it, instead of a continuous value. \n",
    "    - It does so by inserting the weighted sum into a logistic function. \n",
    "    - **Why a logistic/sigmoid function?** If we simply use the weighted sum to assign a probability on an observation, it wouldn't make sense as **probabilities have to fall between [0, 1]**. A logistic function outputs values between [0, 1].  \n",
    "- **How does logistic regression calcuate the weights/coefficients of the regression equation?** \n",
    "    - Finds the set of coefficients/weights such that the probability of the observation belonging to a particular category corresponds as closely to its actual category by maximizing the likelihood or minimizing the cross-entropy loss function. Gradient descent is employed to find the global minimum of that function, and the result is the set of weights/coefficients that best models the probability.\n",
    "\n",
    "### Assumptions\n",
    "- Response variable is binary\n",
    "- Observations are independent, meaning we can maximize the likelihood to find the best fitting set of weights (Optimizer)\n",
    "- There shouldn't be any multicollinearity among variables because when two or more explanatory variables are highly correlated to each other, they don't provide unique or independent information in the regression model\n",
    "- There shouldn't be any extreme outliers \n",
    "- There is a linear relationship between the explanatory variables and the logit\n",
    "\n",
    "### Hyperparameters\n",
    "- When used with L1 or L2 regularization, logistic regression inherits hyperparameter tuning from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVMs)\n",
    "\n",
    "### Motivations\n",
    "- SVMs are effective in high dimensional data\n",
    "- It is still effective even when number of dimensions is greater than the number of samples\n",
    "- It is also memory efficient because SVMs only use a subset of training observations in the decision functions (support vectors)\n",
    "- Versatile: different kernel functions can be used for the decision function to create a linear or non-linear decision boundary\n",
    "\n",
    "### Disadvantages\n",
    "- If number of features is much greater than number of observations, we need to avoid overfitting by choosing the optimal kernel function and regularization term\n",
    "- SVMs don't directly quantify the probability of class assignments\n",
    "\n",
    "### Summary\n",
    "\n",
    "### What is a hyperplane?\n",
    "\n",
    "In a p-dimensional space, a hyperplane is **p-1 dimensional flat affine subspace**. For instance, in a 2 dimensional space, the hyperplane is a line (1 dimensional flat affine subspace). \n",
    "\n",
    "A hyperplane is characterized by the following equation \n",
    "\n",
    "$$\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p = 0$$\n",
    "\n",
    "where $\\beta = (\\beta_0, \\beta_1, \\ldots, \\beta_p)$ are coefficients/weights and $X = (X_1, X_2, ..., X_p)$ is a point in the space. We can also see that this is the inner product of $\\beta$ and $X$/weighted sum of the features constrained to being equal to 0. \n",
    "\n",
    "Now, if the inner product is $> 0$, this indicates that $X$ lies on one side of the hyperplane and vice versa. We can think of the hyperplane as dividing the p-dimensional space into 2 halves. \n",
    "\n",
    "With this property, we can define a **separating hyperplane** to classify points based on which side of the hyperplane it is located, which is characterized by the sign of the inner product result. \n",
    "\n",
    "If the target value of an observation is 1, then the inner product is $> 0$ and if the target value is -1, the inner product is $< 0$. This gives way to the property\n",
    "\n",
    "$$y_i(\\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\ldots + \\beta_pX_{ip}) > 0$$\n",
    "\n",
    "We can also utilize the magnitude of inner product–a value far from 0 means it lies far from the hyperplane and we can be confident about our class prediction, and a value closer to 0 means we are more uncertain about our prediction. \n",
    "\n",
    "### Maximal Margin Classifier\n",
    "\n",
    "Data can be separated by an infinite number of hyperplanes because they can be shifted or rotated without changing anything. \n",
    "\n",
    "To choose the hyperplane to separate the data most accurately, we can use the maximal margin hyperplane. The margin is the smallest perpendicular distance between the observations and hyperplane. \n",
    "\n",
    "With that, the maximal margin hyperplane is the separating hyperplane where the margin is the largest/has the farthest minimum distance to the observations. In an easier sense, we can think of the maximal margin hyperplane to be the midline on the widest \"slab\" that we can insert between 2 classes on the data. We can then use this separating hyperplane as our maximal margin classifier.  \n",
    "\n",
    "#### How can we construct the maximal margin classifier?\n",
    "\n",
    "For a set of n observations $x_1, \\ldots, x_n \\in \\R^p$ and is associated with class labels $y_1, \\ldots, y_n \\in {-1, 1}$, the solution to the optimization problem for the maximal margin is \n",
    "\n",
    "$$\n",
    "\\text{maximize}_{\\beta_0, \\ldots, \\beta_p, M} M \\\\\n",
    "\\text{subject to} \\sum_{j = 1}^{p}\\beta_{j}^2 = 1 \\\\\n",
    "y_i(\\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\ldots + \\beta_pX_{ip}) \\geq M \\forall i = 1, \\ldots, n \n",
    "$$\n",
    "\n",
    "Here, $M$ is the margin. Hence, the first equation states that we want to maximize the margin by optimizing on the $\\beta$\n",
    "\n",
    "The last constraint guarantees that each observation will be on the correct side of the hyperplane, as in the inner product value that's at least greater than the margin can either be on the margin or hyperplane of the correct side. This is from the property of the separating hyperplane where $y_i(\\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\ldots + \\beta_pX_{ip}) > 0$ where we were guaranteeing that the observation would simply be on the right side of the hyperplane. By making it $\\geq M$, we are making it stricter and saying the observation will also be on the right side of the margin. \n",
    "\n",
    "The second constraint allows the inner product in the last constraint to be the perpendicular distance between each observation and separating hyperplane. \n",
    "\n",
    "Together, the solution allows us to guarantee that each observation will be on the correct side of the hyerplane and at least a distance M from the hyerplane. \n",
    "\n",
    "### Support Vector Classifiers / Soft Margin Classifier\n",
    "\n",
    "There are many cases where a separable hyperplane won't exist, and even if there was a classifier that perfectly classifies all of the observations can lead to an overfitting classifier, becoming extremely sensitive to a change in the data. \n",
    "\n",
    "The support vector classifier is the generalization of the maximal margin classifier where we use a hyperplane that *almost* separates the classes, using a **soft margin**. This soft margin allows some observations to be on the incorrect side of the margin or hyperplane. \n",
    "\n",
    "#### How can we construct the support vector classifier / soft margin classifier?\n",
    "\n",
    "$$\n",
    "\\text{maximize}_{\\beta_0, \\ldots, \\beta_p, \\epsilon_1, \\ldots, \\epsilon_n, M} M \\\\\n",
    "\\text{subject to} \\sum_{j = 1}^{p}\\beta_{j}^2 = 1 \\\\\n",
    "y_i(\\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\ldots + \\beta_pX_{ip}) \\geq M(1 - \\epsilon_i) \\forall i = 1, \\ldots, n \\\\\n",
    "\\epsilon_i \\geq 0, \\sum_{i = 1}^{n}\\epsilon_i \\leq C\n",
    "$$\n",
    "\n",
    "$\\epsilon_1, \\ldots, \\epsilon_n$ are slack variables that allow observations to be on the wrong side of the margin or hyperplane as it describes where each observation is located relative to the hyperplane and margin. \n",
    "\n",
    "If $\\epsilon_i = 0$, the observation is on the correct side of the margin, if $\\epsilon_i > 0$, the observation is on the wrong side of the margin and if $\\epsilon_i > 1$, the observation is on the wrong side of the hyperplane. \n",
    "\n",
    "$C$ is a tuning parameter, which bounds the sum of the slack variables. It determines the number and severity of the violations to the margin (and hyperplane) that we tolerate.\n",
    "\n",
    "For $C = 0$, $\\epsilon_1 = \\ldots = \\epsilon_n = 0$, which is the maximal margin hyperplane classifier. For $C > 0$, no more than $C$ observations can be on the wrong side of the hyperplane. As C increases we become more tolerant of violations to the margin and the margin will widen and vice versa. \n",
    "\n",
    "In the solution of the optimization problem above, actually only observations that lie on the marin or that violate the margin will affect the hyperplane and the classifier. These observations are called **support vectors**. \n",
    "\n",
    "Because the support vector classifier only depends on a potentially small subset of observations, it implies that the classifier is robust to observations that are far away from the hyperplane (outliers). \n",
    "\n",
    "#### C + Bias-Variance Trade-off\n",
    "Like many other tuning parameters, $C$ controls the bias-variance trade-off. When $C$ is small, we get narrow margins where not many observations violate them, but we get a classifier that is highly fit to the data and can lead to a classifier that has low bias and high variance and vice versa. This means we want to choose a $C$ that doesn't lead our model to overfit or underfit. \n",
    "\n",
    "### Support Vector Machine (SVM)\n",
    "\n",
    "The support vector classifier and maximal margin classifier are linear clasifiers that product linear decision boundaries. The support vector machine is a more generalized version of these 2 classifiers that supports non-linear decision boundaries. \n",
    "\n",
    "For data that work better with non-linear boundaries, we can enlarge the feature space using functions of predictors with **kernels**. \n",
    "\n",
    "#### Linear SVC\n",
    "\n",
    "The solution to the support vector classifier is just the inner products of the observations, not simply just the observations. The linear support vector classifier is \n",
    "\n",
    "$$\n",
    "f(x) = \\beta_0 + \\sum_{i = 1}^{n}\\alpha_i\\langle x, x_i \\rangle\n",
    "$$\n",
    "\n",
    "We can think of the inner product between $x$ (test observation) and $x_i$ (training observation) as the similarity between each other. To estimate $\\alpha_i$, all we need is just the inner products of the observations. \n",
    "\n",
    "$\\alpha_i$ is actually non-zero only for support vectors in the solution. This makes sense because only support vectors affect the classifier. Thus, it is only necessary we know the inner product/similarity between $x$ and the support vectors to know which side it should be on/class assignment. \n",
    "\n",
    "Take S as the collection of indices of the support vectors, we can rewrite the equation as \n",
    "\n",
    "$$\n",
    "f(x) = \\beta_0 + \\sum_{i \\in S}\\alpha_i\\langle x, x_i \\rangle\n",
    "$$\n",
    "\n",
    "Overall, the inner products of the observations represent the linear support vector classifier and computing the coefficients $\\alpha_i$ or $\\beta_i$.\n",
    "\n",
    "#### Non-Linear SVC/SVM\n",
    "\n",
    "Now instead, of the inner product representing which side the observation should be on, we can use the kernel which is function that quantifies the similarity of 2 observations. The non-linear support vector classifier/support vector machine is characterized as\n",
    "\n",
    "$$\n",
    "f(x) = \\beta_0 + \\sum_{i \\in S}\\alpha_iK(x, x_i)\n",
    "$$\n",
    "\n",
    "If K is simply the inner product/Pearson standard correlation (linear kernel), we get the support vector classifier. If we expand from that and use a polynomial kernel, we get more flexible decision boundaries. \n",
    "\n",
    "Rather than simply enlarging the feature space using functions of original features, we don't need to work with a lot of dimensions and only need to compute the kernel values for all $n \\choose 2$ distinct pairs of observations. \n",
    "\n",
    "### Relationship to Logistic Regression\n",
    "\n",
    "### Practical Procedure\n",
    "\n",
    "#### Data Preprocessing\n",
    "\n",
    "- SVM requires that each observation is represented by a vector of real numbers. For categorical features, we need to encode them as one-hot numbers or integers to convert them to real numbers\n",
    "\n",
    "- Scaling before SVM is very important as to avoid attributes in greater numeric ranges dominating those in smaller numeric ranges. It is also to avoid numerical difficulties during calculations. Since kernel values depend on inner product of feature vectors, large attribute values or the fact that they're even differing ranges can cause problems. \n",
    "\n",
    "#### Model Selection\n",
    "\n",
    "RBF kernel is a good first choice as it nonlinearly maps samples into higher dimensional spaces to support relationships between features that are nonlinear. However, we may want to avoid it when we're dealing with high dimensional data. \n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Depending on the kernel we use, there can be different hyperparameters. However, the main hyperparameter that we have to tune is $C$, the maximum number of observations that are allowed to violate the margin/tolerance for violation of the margin. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN)\n",
    "\n",
    "### Motivations\n",
    "- Can make highly accurate predictions\n",
    "- Intuitive to understand\n",
    "- Can learn non-linear decision boundaries for classification\n",
    "- No training step required, all the work happens during prediction of class labels\n",
    "- Simple hyperparameter tuning. Only need to tune K (number of nearest neighbors to account for). \n",
    "- KNN is a non-parametric algorithm. This means it is a good algorithm to use when we have little to no information on the data distribution. \n",
    "\n",
    "### Disadvantages \n",
    "- Has high complexity for large datasets and high-dimensional data\n",
    "- It assumes equal importance across all features. This also means that KNN is sensitive if the features have different ranges and that we should avoid unrelated features. \n",
    "- It is sensitive to outliers. Because class labels are assigned on the basis of the distance between the main data point and its K neighbors, having outliers can significantly affect the class assignments and cause the result to be more inaccurate. Outliers also affect KNN when the dataset has many dimensions. The average separation tends to be higher for high dimensions, and this impacts the final result. \n",
    "- KNN doesn't perform well on imbalanced datasets. If there is a class majority, the algorithm is bound to give more preference to A. \n",
    "- It can't deal with missing values. \n",
    "- Redundancy affects performance. If 2 or more variables are correlated with each other, the algorithm will place less weights on them without any legitimate reason to actually do that. \n",
    "\n",
    "### Hyperparameters\n",
    "- K: Number of nearest neighbors a data point takes into account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "### Bagging - Random Forest\n",
    "\n",
    "### Boosting\n",
    "#### AdaBoost\n",
    "#### Gradient Boosting\n",
    "#### Extreme Gradient Boosting (XGBoost)\n",
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating classifiers\n",
    "\n",
    "When evaluating classifiers, we can employ validation or cross-validation to validate our model and assess how it performs on a dataset that's closer to future/unknown data. \n",
    "\n",
    "However, another big problem plagues the evaluation of classifiers: Class imbalance. \n",
    "\n",
    "Imagine we made a naive classifier for detecting spam where it always predicts an email is spam and that our dataset is actually 90% of spam emails. The accuracy of the classifier will be 90% because most of the emails are actually spam. This is called a **majority class classifier**, a simple classifier that can have a high accuracy if there is a class imbalance. \n",
    "\n",
    "Hence, accuracy might not be the most robust and reliable metric to evaluate a classifier. That's why we have other ways of evaluating them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix - Precision/Recall\n",
    "\n",
    "![alt text](https://glassboxmedicine.files.wordpress.com/2019/02/confusion-matrix.png?w=816)\n",
    "\n",
    "For a binary classifier, each row represents an **predicted class** and each column represents a **actual class**. A confusion matrix gives information on correctly and incorrectly predicted values with reference to values' actual classes in a summarized form. \n",
    "\n",
    "From a confusion matrix, we can derive **precision**: correctly predicted positives/all predicted positives and **recall/sensitivity/true positive rate (TPR)**: correctly predicted positives/all actual positives. \n",
    "\n",
    "![alt text](https://miro.medium.com/max/1400/1*yH2SM0DIUQlEiveK42NnBg.png)\n",
    "\n",
    "For multiclass classifier, we can obtain the same information as a confusion matrix for a binary classifier, but **for each class**. This means we can find the true/false positives/negatives for each class and we can transform the multiclass problem to a binary problem by thinking in a **One-versus-Rest** way. \n",
    "\n",
    "Focusing on the Apple class, TP = 7, TN = (2+3+2+1) = 8, FP = (8+9) = 17, FN = (1+3) = 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs K-fold cross-validation but will return the predicted made on each test fold\n",
    "y_train_pred = model_selection.cross_val_predict(estimator, X_train, y_train, cv)\n",
    "metrics.confusion_matrix(y_train, y_train_pred)\n",
    "\n",
    "# Calculate precision and recall score\n",
    "metrics.precision_score(y_train, y_train_pred)\n",
    "metrics.recall_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall - F1 Score\n",
    "\n",
    "Sometimes, it can be convenient to combine precision and recall into a single metric–**F1 score**, particularly if we need a simple way to **compare 2 classifiers**. \n",
    "\n",
    "F1 score favors classifiers that have similar precision and recall, so it can be fitting to use it when we want to optimize for both precision and recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision/Recall Trade-off\n",
    "\n",
    "Suppose the classifier is deciding whether a potential client is risky or not based on their income against an income threshold. The precision/recall trade-off can be shown when we shift the decision threshold and the precision and recall changes. \n",
    "\n",
    "To visualize this tradeoff, we can plot precision and recall values against threshold or even plot precision against recall. \n",
    "\n",
    "![alt text](https://miro.medium.com/max/1248/1*TqzfzabXrej1FTdZuNNYIQ.png)\n",
    "\n",
    "### Precision-Recall Curve\n",
    "\n",
    "Precision-Recall curves plot precision against recall where the precision and recall values are taken from decision thresholds from either a decision function or probability estimates of the positive class.\n",
    "\n",
    "The curve allows us to choose a decision threshold that defines the precision/recall tradeoff we make. \n",
    "\n",
    "![alt text](https://machinelearningmastery.com/wp-content/uploads/2020/01/Precision-Recall-Curve-of-a-Logistic-Regression-Model-and-a-No-Skill-Classifier2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives list of precision, recall and threshold values that can be used to plot a precision-recall curve\n",
    "# y_scores can be scores from a decision function or probability estimates of the positive class\n",
    "precisions, recalls, thresholds = metrics.precision_recall_curve(y_train, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve\n",
    "\n",
    "Receiver operating characteristic curves are like precision-recall curves but they plot the true positive rate (**recall**) against the false positive rate, the ratio of negative instances that are incorrectly classified as positive (1 - true negative rate (specificity)). \n",
    "\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/Roc_curve.svg/1920px-Roc_curve.svg.png)\n",
    "\n",
    "One way to compare classifiers is to measure the area under the curve (AUC). A perfect classifier will have an AUC of 1 whereas a random classifier will have a ROC of 0.5. \n",
    "\n",
    "### ROC Curve vs. Precision-Recall (PR) Curve?\n",
    "Rule of thumb, PR curve is prefered when there aren't a lot of positive class values or when we care more about false positives than false negatives. Otherwise, use ROC curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives list of fpr, tpr and threshold values that can be used to plot a ROC curve\n",
    "# y_scores can be scores from a decision function or probability estimates of the positive class\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "### The main idea of gradient descent is to tweak the parameters iteratively to minimize a cost function.\n",
    "\n",
    "1. General Idea\n",
    "2. Batch Gradient Descent\n",
    "3. Stochastic Gradient Descent\n",
    "4. Mini-batch Gradient Descent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('pax_satisfaction')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4c148714c4b36448fc626c7d0cced70895e17adbe163c475e562080f31f1b22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
